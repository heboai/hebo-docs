---
title : Hebo Eval - Hebo Documentation
---

## Hebo Eval CLI

Hebo Eval is a powerful command-line tool for evaluating and testing language models. It provides a robust framework for running evaluations against your AI agents and generating detailed reports.

### Installation

```bash
npm install -g hebo-eval@latest
```

### Basic Usage

```bash
hebo-eval run <agent> [options]
```

### Command Options

| Option | Description | Default |
|--------|-------------|---------|
| `-d, --directory <path>` | Directory containing test cases | `./examples` |
| `-c, --config <path>` | Path to configuration file | - |
| `-t, --threshold <number>` | Score threshold for passing (0-1) | `0.8` |
| `-f, --format <format>` | Output format (json\|markdown\|text) | `text` |
| `-s, --stop-on-error` | Stop processing on first error | `false` |
| `-m, --max-concurrency <number>` | Maximum number of concurrent test executions | `5` |
| `--verbose` | Show detailed output for all test cases | `false` |

### Configuration

You can configure Hebo Eval in two ways:

1. **Environment Variables**:
   ```bash
   export HEBO_AGENT_API_KEY=your_api_key_here
   ```

2. **Configuration File**:
   Create a JSON configuration file (e.g., `config.json`):
   ```json
   {
     "embedding": {
       "provider": "hebo",
       "model": "hebo-embedding",
       "apiKey": "your_hebo_embeddings_key",
     },
     "agent": {
       "agentKey": "your_hebo_agent_key",
       "provider": "hebo"
     }
   }
   ```

### Test Cases

Hebo Eval supports a flexible test case structure that allows you to organize and manage your test cases effectively.

#### Test Case Structure

1. **Multiple Test Cases in One File**:
   - Test cases can be defined in a single file, separated by `---`
   - Each test case starts with a title using `# Test Case Name` format

2. **Directory Organization**:
   - Test cases can be organized in subdirectories
   - All test cases in subdirectories are automatically discovered and executed

#### Test Case Format

```text
# Basic Conversation Test
System: You are a helpful assistant that specializes in customer service.
User: Hi there!
Assistant: Hello! How can I assist you today?

---
# Weather Query Test
System: You are a weather assistant that provides detailed weather information.
User: Could you check the current weather in New York for me?
Assistant: It's rainy in New York today with a temperature of 59°F. There's an 80% chance of rain, high humidity (96%), and a light breeze at 2 mph. You might want to bring an umbrella!
```

#### Directory Structure Example

```
tests/
├── basic/
│   ├── conversations.txt
│   └── simple_queries.txt
├── advanced/
│   ├── tool_usage.txt
│   └── complex_scenarios.txt
└── main.txt
```

### Output Format

The tool supports three output formats with different verbosity levels:

#### Default Output (Concise)
```
Passed examples/more tests/test/Silly math
Passed examples/example/First Test Case
Passed examples/more tests/test/Math
Passed examples/math/math
Passed examples/example/Second Test Case
Passed examples/example/Third Test Case
Failed examples/stocks/stocks
Passed examples/news/news
Passed examples/translation/translation
Passed examples/weather/weather

Failed Test Details
=================
examples/stocks/stocks
Status: Failed
Score: 0.398
Time: 16694.51ms

Input:
user: what's the current price of Apple stock?
assistant: I'll check the current stock price
Apple's stock (AAPL) is currently trading at USD175.25, up 2.3 percent today
user: can you write that again in simple terms?

Expected Output:
assistant: something something someting in simple terms

Actual Response:
Sure, I can rephrase that in simpler terms:

Apple's shares cost $175.25 each right now. The price went up a bit today.

Error:
Response mismatch

Test Summary
============
Total: 10
Passed: 9
Failed: 1
Duration: 50.54s
```


### Example Usage

1. **Basic Evaluation**:
   ```bash
   hebo-eval run gato-qa:v1
   ```

2. **Custom Directory and Format**:
   ```bash
   hebo-eval run gato-qa:v1 -d ./my-tests -f markdown
   ```

3. **With Configuration File**:
   ```bash
   hebo-eval run gato-qa:v1 -c ./config.json
   ```

4. **Custom Threshold and Concurrency**:
   ```bash
   hebo-eval run gato-qa:v1 -t 0.5 -m 10
   ```

5. **Verbose Output**:
   ```bash
   hebo-eval run gato-qa:v1 --verbose
   ```

### Best Practices

1. Always set up your API keys before running evaluations
2. Start with a small test set before running large evaluations
3. Use descriptive test case titles with the `#` format
4. Organize test cases in subdirectories for better management
5. Keep your configuration file secure and never commit it to version control
6. Use the `--verbose` flag when debugging test failures

### Troubleshooting

If you encounter the "HEBO_API_KEY is required" error:

1. Verify your environment variables:
   ```bash
   export HEBO_API_KEY=your_api_key_here
   ```

2. Or use a configuration file:
   ```bash
   hebo-eval run <agent> --config path/to/config.json
   ```


